#!/bin/bash --login

# PBS job options (name, compute nodes, job time)
#PBS -N Example_Job
# Select 4 full nodes
#PBS -l select=1:ncpus=36
# Parallel jobs should always specify exclusive node access
#PBS -l place=scatter:excl
#PBS -l walltime=00:20:00

# Replace [budget code] below with your project code (e.g. t01)
#PBS -A dc010-jfmunoz

# Change to the directory that the job was submitted from
cd $PBS_O_WORKDIR

# Load any requ_ired modules
module load gcc/6.3.0
module load cmake/3.14.1
module load intel-tbb/16.0.3.210
module load fftw/3.3.5-gcc6
module load anaconda/python3

#set env. variables
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lustre/home/dc010/jfmunoz/test/libpng_install/lib64/

# activate conda enviroment
source activate dispel4py-env

# Set the number of threads to 1
#   This prevents any threaded system libraries from automatically
#   using threading.
export OMP_NUM_THREADS=1

# Launch the parallel job
#   Using 144 MPI processes and 36 MPI processes per node
#
#   '-ppn' option is required for all HPE MPT jobs otherwise you will get an error similar to:
#       'mpiexec_mpt error: Need 36 processes but have only 1 left in PBS_NODEFILE.'
#
set -x

  export PYTHONPATH=$PYTHONPATH:.

  export GRPPI_DIR=/lustre/home/dc010/jfmunoz/test/dispel4py_GrPPI_workflows/grppi_workflows/build/use_cases/tc_cross_correlation/
  export OUTPUT_DIR=/lustre/home/dc010/jfmunoz/test/dispel4py_GrPPI_workflows/dispel4py_workflows/tc_cross_correlation/OUTPUT2/

  export DISPEL4PY_XCORR_STARTTIME=2019-08-07T06:00:00.000
  export DISPEL4PY_XCORR_ENDTIME=2019-08-07T07:00:00.000

  rm -rf ${OUTPUT_DIR}/DATA
  rm -rf ${OUTPUT_DIR}/XCORR
  mkdir ${OUTPUT_DIR}/DATA
  mkdir ${OUTPUT_DIR}/XCORR

  dispel4py multi tc_cross_correlation/realtime_prep.py -f tc_cross_correlation/realtime_xcorr_input.jsn -n 36
  #dispel4py multi tc_cross_correlation/realtime_xcorr_mod.py -n 36
  taskset -c 0-35 ${GRPPI_DIR}/tc_cross_correlation $OUTPUT_DIR $DISPEL4PY_XCORR_STARTTIME tbb
